{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing get_wid2cid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_wid2cid.py\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "#import torch.nn.utils\n",
    "from vocab import Vocab, VocabEntry\n",
    "\n",
    "def gen_wid2cid():\n",
    "    #char_list = list(\"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|\n",
    "    _@#$%^&*~`+-=<>()[]\"\"\")\n",
    "    char_list = list(\"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|\n",
    "    _@#$%^&*~`+-=<>()[]\"\"\")\n",
    "    char_list += ['”','“','—','’','˅','‘','é','…','–','•','á','·','í','ʹ','ó','ü','°','´']\n",
    "    char2id = dict() # Converts characters to integers\n",
    "    char2id['<pad>'] = 0\n",
    "\n",
    "    for i, c in enumerate(char_list):\n",
    "        char2id[c] = len(char2id)\n",
    "\n",
    "    char_size = len(char2id)\n",
    "\n",
    "    vocab = Vocab.load('vocab.json')\n",
    "\n",
    "    wid2cid = {}\n",
    "    for i in range(len(vocab.vocs)):\n",
    "        w = [char2id[c] if c in char2id.keys() else 0 for c in vocab.vocs.id2word[i]]\n",
    "        wid2cid[i] = w[:10] + [0]*(10-len(w))\n",
    "    json.dump(wid2cid, open('wid2cid.json', 'w'), indent=2)\n",
    "    \n",
    "    return wid2cid, char_size\n",
    "\n",
    "###########################################\n",
    "class CharEmbeddings(nn.Module): \n",
    "\n",
    "    def __init__(self, char_size):\n",
    "\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "        self.char_embeddings = nn.Embedding(char_size, 30)\n",
    "        self.projection = nn.Linear(300, 300, bias=False)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.projection(self.char_embedding(torch.tensor(X)).view(-1,300).contiguous)\n",
    "        return X\n",
    "\n",
    "upper_index = torch.tensor((mask == 1))\n",
    "upperX = [[wid2cid[str(w)] for w in ws] for ws in batch[upper_index]]\n",
    "self.char_emb = CharEmbeddings(char_size)\n",
    "upperX = self.char_emb(upperX)\n",
    "X = torch.zeros(batch_size, emb_size).float().to(device)\n",
    "X[upper_index] = upperX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_embeddings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_embeddings.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharEmbeddings(nn.Module): \n",
    "\n",
    "    def __init__(self, char_size, embed_size, hidden_size):\n",
    "        super(CharEmbeddings, self).__init__()\n",
    "        self.char_embeddings = nn.Embedding(char_size, 30)\n",
    "        self.projection = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.projection(self.char_embeddings(torch.tensor(X)).view(-1,embed_size).contiguous)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
