{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_model import NMT\n",
    "import torch\n",
    "params = torch.load('model_0707.bin', map_location=lambda storage, loc: storage)\n",
    "args = params['args']\n",
    "model = NMT(vocab=params['vocab'], **args)\n",
    "model.load_state_dict(params['state_dict'])\n",
    "\n",
    "torch.save(params['state_dict'], 'model_bi_0707')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['args', 'vocab', 'state_dict'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(553, 198, 2850, 593)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['vocab'].vocs['king'], params['vocab'].vocs['man'], params['vocab'].vocs['queen'], params['vocab'].vocs['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 3, 2, 1]), tensor([3, 1, 2, 0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.tensor([1,3,2,4])\n",
    "aa.sort(0,descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing net_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile net_util.py\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_nn_avg_dist(emb, query, knn):\n",
    "    \"\"\"\n",
    "    Compute the average distance of the `knn` nearest neighbors\n",
    "    for a given set of embeddings and queries.\n",
    "    Use Faiss if available.\n",
    "    \"\"\"\n",
    "    FAISS_AVAILABLE = False\n",
    "    \n",
    "    if FAISS_AVAILABLE:\n",
    "        emb = emb.cpu().numpy()\n",
    "        query = query.cpu().numpy()\n",
    "        if hasattr(faiss, 'StandardGpuResources'):\n",
    "            # gpu mode\n",
    "            res = faiss.StandardGpuResources()\n",
    "            config = faiss.GpuIndexFlatConfig()\n",
    "            config.device = 0\n",
    "            index = faiss.GpuIndexFlatIP(res, emb.shape[1], config)\n",
    "        else:\n",
    "            # cpu mode\n",
    "            index = faiss.IndexFlatIP(emb.shape[1])\n",
    "        index.add(emb)\n",
    "        distances, _ = index.search(query, knn)\n",
    "        return distances.mean(1)\n",
    "    else:\n",
    "        bs = 1024\n",
    "        all_distances = []\n",
    "        emb = emb.transpose(0, 1).contiguous()\n",
    "        for i in range(0, query.shape[0], bs):\n",
    "            distances = query[i:i + bs].mm(emb)\n",
    "            best_distances, _ = distances.topk(knn, dim=1, largest=True, sorted=True)\n",
    "            all_distances.append(best_distances.mean(1).cpu())\n",
    "        all_distances = torch.cat(all_distances)\n",
    "        return all_distances\n",
    "\n",
    "\n",
    "def get_candidates(emb1, emb2, params):\n",
    "    \"\"\"\n",
    "    Get best translation pairs candidates.\n",
    "    \"\"\"\n",
    "\n",
    "    bs = 128\n",
    "\n",
    "    all_scores = []\n",
    "    all_targets = []\n",
    "\n",
    "    # number of source words to consider\n",
    "    n_src = emb1.size(0)\n",
    "    if params['dico_max_rank'] > 0 and not params['dico_method'].startswith('invsm_beta_'):\n",
    "        n_src = min(params['dico_max_rank'], n_src)\n",
    "\n",
    "\n",
    "    # contextual dissimilarity measure\n",
    "    if params['dico_method'].startswith('csls_knn_'):\n",
    "\n",
    "        knn = params['dico_method'][len('csls_knn_'):]\n",
    "        assert knn.isdigit()\n",
    "        knn = int(knn)\n",
    "\n",
    "        # average distances to k nearest neighbors\n",
    "        average_dist1 = get_nn_avg_dist(emb2, emb1, knn)\n",
    "        average_dist2 = get_nn_avg_dist(emb1, emb2, knn)\n",
    "        #average_dist1 = torch.from_numpy(get_nn_avg_dist(emb2, emb1, knn))\n",
    "        #average_dist2 = torch.from_numpy(get_nn_avg_dist(emb1, emb2, knn))\n",
    "        #print('check_point_1')\n",
    "        #average_dist1 = average_dist1.type_as(emb1)\n",
    "        #print('check_point_2')\n",
    "        #average_dist2 = average_dist2.type_as(emb2)\n",
    "\n",
    "        # for every source word\n",
    "        for i in tqdm(range(0, n_src, bs)):\n",
    "\n",
    "            # compute target words scores\n",
    "            scores = emb2.mm(emb1[i:min(n_src, i + bs)].transpose(0, 1)).transpose(0, 1)\n",
    "            scores.mul_(2)\n",
    "            scores.sub_(average_dist1[i:min(n_src, i + bs)].unsqueeze(1).expand_as(scores) \n",
    "                        + average_dist2.unsqueeze(0).expand_as(scores))\n",
    "            best_scores, best_targets = scores.topk(2, dim=1, largest=True, sorted=True)\n",
    "\n",
    "            # update scores / potential targets\n",
    "            all_scores.append(best_scores.cpu())\n",
    "            all_targets.append(best_targets.cpu())\n",
    "\n",
    "        all_scores = torch.cat(all_scores, 0)\n",
    "        all_targets = torch.cat(all_targets, 0)\n",
    "\n",
    "    all_pairs = torch.cat([\n",
    "        torch.arange(0, all_targets.size(0)).long().unsqueeze(1),\n",
    "        all_targets[:, 0].unsqueeze(1)\n",
    "    ], 1)\n",
    "\n",
    "    # sanity check\n",
    "    assert all_scores.size() == all_pairs.size() == (n_src, 2)\n",
    "\n",
    "    # sort pairs by score confidence\n",
    "    diff = all_scores[:, 0] - all_scores[:, 1]\n",
    "    reordered = diff.sort(0, descending=True)[1]\n",
    "    #reordered = all_scores.sort(0, descending=True)[1]\n",
    "    all_scores = all_scores[reordered]\n",
    "    all_pairs = all_pairs[reordered]\n",
    "    \"\"\"\n",
    "    # max dico words rank\n",
    "    if params['dico_max_rank'] > 0:\n",
    "        selected = all_pairs.max(1)[0] <= params['dico_max_rank']\n",
    "        mask = selected.unsqueeze(1).expand_as(all_scores).clone()\n",
    "        all_scores = all_scores.masked_select(mask).view(-1, 2)\n",
    "        all_pairs = all_pairs.masked_select(mask).view(-1, 2)\n",
    "\n",
    "    # max dico size\n",
    "    if params['dico_max_size'] > 0:\n",
    "        all_scores = all_scores[:params['dico_max_size']]\n",
    "        all_pairs = all_pairs[:params['dico_max_size']]\n",
    "    \n",
    "    # min dico size\n",
    "    diff = all_scores[:, 0] - all_scores[:, 1]\n",
    "    if params['dico_min_size'] > 0:\n",
    "        diff[:params['dico_min_size']] = 1e9\n",
    "\n",
    "    # confidence threshold\n",
    "    if params['dico_threshold'] > 0:\n",
    "        mask = diff > params['dico_threshold']\n",
    "        logger.info(\"Selected %i / %i pairs above the confidence threshold.\" % (mask.sum(), diff.size(0)))\n",
    "        mask = mask.unsqueeze(1).expand_as(all_pairs).clone()\n",
    "        all_pairs = all_pairs.masked_select(mask).view(-1, 2)\n",
    "    \"\"\"\n",
    "    return all_pairs\n",
    "\n",
    "\n",
    "def build_dictionary(src_emb, tgt_emb, params, s2t_candidates=None, t2s_candidates=None):\n",
    "    \"\"\"\n",
    "    Build a training dictionary given current embeddings / mapping.\n",
    "    \"\"\"\n",
    "    #logger.info(\"Building the train dictionary ...\")\n",
    "    s2t = 'S2T' in params['dico_build']\n",
    "    t2s = 'T2S' in params['dico_build']\n",
    "    assert s2t or t2s\n",
    "\n",
    "    if s2t:\n",
    "        if s2t_candidates is None:\n",
    "            s2t_candidates = get_candidates(src_emb, tgt_emb, params)\n",
    "    if t2s:\n",
    "        if t2s_candidates is None:\n",
    "            t2s_candidates = get_candidates(tgt_emb, src_emb, params)\n",
    "        t2s_candidates = torch.cat([t2s_candidates[:, 1:], t2s_candidates[:, :1]], 1)\n",
    "\n",
    "    if params['dico_build'] == 'S2T':\n",
    "        dico = s2t_candidates\n",
    "    elif params['dico_build'] == 'T2S':\n",
    "        dico = t2s_candidates\n",
    "    else:\n",
    "        s2t_candidates = set([(a, b) for a, b in s2t_candidates.numpy()])\n",
    "        t2s_candidates = set([(a, b) for a, b in t2s_candidates.numpy()])\n",
    "        if params['dico_build'] == 'S2T|T2S':\n",
    "            final_pairs = s2t_candidates | t2s_candidates\n",
    "        else:\n",
    "            assert params['dico_build'] == 'S2T&T2S'\n",
    "            final_pairs = s2t_candidates & t2s_candidates\n",
    "            if len(final_pairs) == 0:\n",
    "                #logger.warning(\"Empty intersection ...\")\n",
    "                return None\n",
    "        dico = torch.LongTensor(list([[int(a), int(b)] for (a, b) in final_pairs]))\n",
    "\n",
    "    #logger.info('New train dictionary of %i pairs.' % dico.size(0))\n",
    "    return dico   #.cuda() if params['cuda else dico']\n",
    "\n",
    "def dict_merge(d1,d2):\n",
    "    for k,v in d1.items():\n",
    "        if k in d2.keys():\n",
    "            d2[k] += v\n",
    "        else:\n",
    "            d2[k] = v\n",
    "\n",
    "        \n",
    "def save_best(self, to_log, metric):\n",
    "    \"\"\"\n",
    "    Save the best model for the given validation metric.\n",
    "    \"\"\"\n",
    "    # best mapping for the given validation criterion\n",
    "    if to_log[metric] > self.best_valid_metric:\n",
    "        # new best mapping\n",
    "        self.best_valid_metric = to_log[metric]\n",
    "        logger.info('* Best value for \"%s\": %.5f' % (metric, to_log[metric]))\n",
    "        # save the mapping\n",
    "        W = self.mapping.weight.data.cpu().numpy()\n",
    "        path = os.path.join(self.params['exp_path'], 'best_mapping.pth')\n",
    "        logger.info('* Saving the mapping to %s ...' % path)\n",
    "        torch.save(W, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.arange(12).view(3,-1)\n",
    "aa.max(1)\n",
    "selected = aa.max(1)[0] <= 8\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.uint8) tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [4, 5, 6, 7]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = selected.unsqueeze(1).expand_as(aa).clone()\n",
    "print(mask,aa)\n",
    "aa.masked_select(mask).view(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = 'csls_knn_nnn'\n",
    "aa.startswith('csls_knn_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54621, 300])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_emb = net.mapping(net.emb[:net.ko_start])\n",
    "src_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47110, 300])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_emb = net.mapping(net.emb[net.ko_start:])\n",
    "tgt_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.arange(12)\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [2, 3],\n",
       "        [3, 5]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([(1,2),(2,3),(3,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([(1,2),(2,3),(3,5)])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101731, 300])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing net_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile net_model.py\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "#from build_dict import get_candidates, build_dictionary\n",
    "\n",
    "class Net(nn.Module): #Word Mapping\n",
    "\n",
    "    def __init__(self, parameters, hidden_size, dropout_rate=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ko_start = 54621\n",
    "        #self.dropout_rate = dropout_rate\n",
    "        #self.vocab = vocab\n",
    "        #self.token = ['(', ')', ',', \"'\", '\"','_','<s>','</s>']\n",
    "        #self.sbol = ['_','^','`']\n",
    "        self.top_k = 10\n",
    "        self.some_number = 0.3\n",
    "        self.best_valid_metric = 0\n",
    "        self.vocs = parameters['vocab'].vocs\n",
    "        self.emb = parameters['state_dict']['model_embeddings.vocabs.weight']\n",
    "        self.lookup = {}\n",
    "        \n",
    "        self.mapping = None\n",
    "        self.mapping = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  \n",
    "        \n",
    "\n",
    "    def forward(self, bi_dict):\n",
    "        \n",
    "        if type(bi_dict[0][0]) is str:\n",
    "            bi_dict = torch.tensor([(self.vocs[b[0]], self.vocs[b[1]]) for b in bi_dict])\n",
    "        print(\"bi_dict.size : {}\".format(bi_dict.size()))\n",
    "        self.mapping.weight.data = self.procrustes(bi_dict)\n",
    "        mean_cosine, dico = self.dist_mean_cosine()\n",
    "        self.save_best(mean_cosine)\n",
    "       \n",
    "        return dico, mean_cosine\n",
    "           \n",
    "    \n",
    "    def procrustes(self, dico):\n",
    "        \"\"\"\n",
    "        Find the best orthogonal matrix mapping using the Orthogonal Procrustes problem\n",
    "        https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n",
    "        \"\"\"\n",
    "        A = self.emb[dico[:, 0]]\n",
    "        B = self.emb[dico[:, 1]]\n",
    "        W = self.mapping.weight.data\n",
    "        M = B.transpose(0, 1).mm(A).cpu().numpy()\n",
    "        U, S, V_t = scipy.linalg.svd(M, full_matrices=True)\n",
    "        W.copy_(torch.from_numpy(U.dot(V_t)).type_as(W)) \n",
    "        print(\"W.size : {}\".format(W.size()))\n",
    "        return W\n",
    "        \n",
    "    def dist_mean_cosine(self):\n",
    "        \"\"\"\n",
    "        Mean-cosine model selection criterion.\n",
    "        \"\"\"\n",
    "        n_wds = 20000\n",
    "        # get normalized embeddings\n",
    "        #src_emb = self.mapping(self.emb[:self.ko_start])\n",
    "        #tgt_emb = self.emb[self.ko_start:]\n",
    "        src_emb = self.mapping(self.emb[:n_wds])\n",
    "        tgt_emb = self.emb[self.ko_start:self.ko_start+n_wds]\n",
    "        src_emb = src_emb / src_emb.norm(2, 1, keepdim=True) #.expand_as(src_emb)\n",
    "        tgt_emb = tgt_emb / tgt_emb.norm(2, 1, keepdim=True) #.expand_as(tgt_emb)\n",
    "\n",
    "        # build dictionary\n",
    "        for dico_method in ['csls_knn_10']:\n",
    "            dico_build = 'S2T'\n",
    "            dico_max_size = 30000\n",
    "            # temp params / dictionary generation\n",
    "            _params = {}\n",
    "            #_params = deepcopy(self.params)\n",
    "            _params['dico_method'] = dico_method # 'csls_knn_10'\n",
    "            _params['dico_build'] = dico_build\n",
    "            _params['dico_threshold'] = 0\n",
    "            _params['dico_max_rank'] = 30000\n",
    "            _params['dico_min_size'] = 0\n",
    "            _params['dico_max_size'] = dico_max_size\n",
    "            \n",
    "            s2t_candidates = get_candidates(src_emb, tgt_emb, _params)\n",
    "            t2s_candidates = get_candidates(tgt_emb, src_emb, _params)\n",
    "            dico = build_dictionary(src_emb, tgt_emb, _params, s2t_candidates, t2s_candidates)\n",
    "            # mean cosine\n",
    "            if dico is None:\n",
    "                mean_cosine = -1e9\n",
    "            else:\n",
    "                mean_cosine = (src_emb[dico[:dico_max_size, 0]] * tgt_emb[dico[:dico_max_size, 1]]).sum(1).mean()\n",
    "            #mean_cosine = mean_cosine.item() if isinstance(mean_cosine, torch_tensor) else mean_cosine\n",
    "            print(\"Mean cosine (%s method, %s build, %i max size): %.5f\"\n",
    "                        % (dico_method, _params['dico_build'], dico_max_size, mean_cosine))\n",
    "            #to_log['mean_cosine-%s-%s-%i' % (dico_method, _params['dico_build'], dico_max_size)] = mean_cosine\n",
    "            \n",
    "        return mean_cosine, dico\n",
    "        \n",
    "    def save_best(self, metric):\n",
    "        \"\"\"\n",
    "        Save the best model for the given validation metric.\n",
    "        \"\"\"\n",
    "        # best mapping for the given validation criterion\n",
    "        if metric > self.best_valid_metric:\n",
    "            # new best mapping\n",
    "            self.best_valid_metric = metric\n",
    "            #logger.info('* Best value for \"%s\": %.5f' % (metric, to_log[metric]))\n",
    "            # save the mapping\n",
    "            W = self.mapping.weight.data.cpu().numpy()\n",
    "            path = os.path.join('outputs/', 'best_mapping.pth')\n",
    "            print('* Saving the mapping to %s ...' % path)\n",
    "            torch.save(W, path)\n",
    "    \n",
    "    \n",
    "    def make_lookup(self, src, tgt):\n",
    "        for ws in src:\n",
    "            scores, w_topk = torch.topk(cos_sim(self.w_map(self.emb[wD[ws]]).expand(batch,self.hidden_size),\n",
    "                              embed[self.ws2inds(tgt)]), self.top_k)\n",
    "            wt = w_topk[0]\n",
    "            scores_sk, ws_topk = torch.topk(cos_sim(self.emb[wD[ws]].expand(batch,self.hidden_size),\n",
    "                              embed[self.ws2inds(src)]), self.top_k)\n",
    "            scores_tk, wt_topk = torch.topk(cos_sim(self.emb[wt].expand(batch,self.hidden_size),\n",
    "                              embed[self.ws2inds(tgt)]), self.top_k)\n",
    "            CSIL = 2* cos_sim(self.w_map(self.emb[wD[ws]]),self.emb[wt]) - (sum(scores_sk)+ sum(scores_tk)) / self.top_k\n",
    "            if CSIL > self.some_number:\n",
    "                self.lookup[ws].append((wt,CSIL))   \n",
    "        return self.lookup\n",
    "            \n",
    "    def ws2inds(self, words):  \n",
    "        return [self.vocs[w] for w in words]\n",
    "    \n",
    "    def cos_sim(self,a,b):\n",
    "        return sum(a*b)/((sum(a*a)**.5)*(sum(b*b)**.5))\n",
    "\n",
    "    \n",
    "def batch_iter(bi_words,b_size):\n",
    "    batch_size = b_size\n",
    "    batch_num = math.ceil(len(bi_words) / batch_size)\n",
    "    index_array = list(range(len(bi_words)))\n",
    "\n",
    "    np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
    "        examples = [bi_words[idx] for idx in indices]\n",
    "        src = [e[0] for e in examples]\n",
    "        tgt = [e[1] for e in examples]\n",
    "\n",
    "        yield src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa ='a'\n",
    "type(aa) is str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = '../../../preProject/NMT/preproc_Module/inputs/'\n",
    "\n",
    "with open(path+'dict_enT.json','r') as f:\n",
    "    dictE = json.load(f)\n",
    "with open(path+'dict_koT.json','r') as f:\n",
    "    dictK = json.load(f)\n",
    "    \n",
    "from nmt_model import NMT\n",
    "import torch\n",
    "params = torch.load('model_0707.bin', map_location=lambda storage, loc: storage)\n",
    "args = params['args']\n",
    "model = NMT(vocab=params['vocab'], **args)\n",
    "model.load_state_dict(params['state_dict'])\n",
    "\n",
    "torch.save(params['state_dict'], 'model_bi_0707')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "bi_words = [(w,sorted(v, key=lambda x:x[1][0], reverse = True)[0][0]) for w,v in dictE.items()]\n",
    "bi_words += [(sorted(v, key=lambda x:x[1][0], reverse = True)[0][0],w) for w,v in dictK.items()]\n",
    "net = Net(params,300)\n",
    "count = Counter()\n",
    "best = 0\n",
    "patience = 0\n",
    "dictionary = torch.tensor([(net.vocs[b[0]], net.vocs[b[1]]) for b in bi_words])\n",
    "dict =[(w[0],w[1]) for w in dictionary.numpy()]\n",
    "dict_count = Counter(dict+dict)\n",
    "dictionary = [w[0] for w in sorted(dict_count.items(), key=lambda x:x[1], reverse=True)[:5000]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5757, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dictionary).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56520"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.vocs['개인적']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((605, 56520), 2),\n",
       " ((8336, 65100), 2),\n",
       " ((74, 53745), 2),\n",
       " ((58, 54886), 2),\n",
       " ((952, 55309), 2),\n",
       " ((486, 55226), 2),\n",
       " ((5420, 59024), 2),\n",
       " ((7833, 63406), 2),\n",
       " ((8827, 64383), 2),\n",
       " ((250, 54874), 2)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20700"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Counter(list(dictionary)+list(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10350"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict =[(w[0],w[1]) for w in dictionary.numpy()]\n",
    "len(Counter((dict+dict)))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count(tensor([[  1210, 113040],\n",
       "        [ 16672, 130200],\n",
       "        [   148, 107490],\n",
       "        ...,\n",
       "        [119892,  70600],\n",
       "        [122294,  25284],\n",
       "        [119374,  23838]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_merge(d1,d2):\n",
    "    for k,v in d1.items():\n",
    "        if k in d2.keys():\n",
    "            d2[k] += v\n",
    "        else:\n",
    "            d2[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing net_run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile net_run.py\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "import json\n",
    "path = '../../../preProject/NMT/preproc_Module/inputs/'\n",
    "\n",
    "with open(path+'dict_enT.json','r') as f:\n",
    "    dictE = json.load(f)\n",
    "with open(path+'dict_koT.json','r') as f:\n",
    "    dictK = json.load(f)\n",
    "    \n",
    "from nmt_model import NMT\n",
    "import torch\n",
    "params = torch.load('model_0707.bin', map_location=lambda storage, loc: storage)\n",
    "args = params['args']\n",
    "model = NMT(vocab=params['vocab'], **args)\n",
    "model.load_state_dict(params['state_dict'])\n",
    "\n",
    "torch.save(params['state_dict'], 'model_bi_0707')\n",
    "\n",
    "bi_words = [(w,sorted(v, key=lambda x:x[1][0], reverse = True)[0][0]) for w,v in dictE.items()]\n",
    "bi_words += [(sorted(v, key=lambda x:x[1][0], reverse = True)[0][0],w) for w,v in dictK.items()]\n",
    "bi_words = list(set(bi_words))\n",
    "net = Net(params,300)\n",
    "\n",
    "best = 0\n",
    "patience = 0\n",
    "dict_count = {}\n",
    "\n",
    "dictionary = torch.tensor([(net.vocs[b[0]], net.vocs[b[1]]) for b in bi_words[:3000]])\n",
    "count = Counter([(w[0],w[1]) for w in dictionary.numpy()])\n",
    "dict_merge(count,dict_count)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    pre_dic = dictionary    \n",
    "    dictionary, m_cosine = net(dictionary)\n",
    "    \n",
    "    print(\"iteration {}, m_cosine = {}\".format(i,m_cosine))\n",
    "    \n",
    "    if m_cosine > best:\n",
    "        best = m_cosine\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        print(\"patience : {}\".format(patience))\n",
    "        if patience >2:\n",
    "            print(\"patience : {},  STOP Iteration\".format(patience))            \n",
    "            break\n",
    "\n",
    "            \n",
    "    dict_count = Counter(chain(*[[(w[0],w[1]) for w in dct.numpy()] for dct in [pre_dic, dictionary[:2000]]]))\n",
    "    #count = Counter([(w[0],w[1]) for w in dictionary.numpy()])\n",
    "    #dict_merge(count,dict_count)\n",
    "    dictionary = torch.tensor([w[0] for w in sorted(dict_count.items(), key=lambda x:x[1], reverse=True)])\n",
    "\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12751"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bi_dict.size : torch.Size([6717, 2])\n",
      "W.size : torch.Size([300, 300])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c3bdc6896d4661bad0b43874080cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f607891810422fa92e945821c99b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "check_point_1\n",
      "check_point_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e90bdc552448dd9f776551491eb924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5fb4f389384cac80ea7f8a5aa1cde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929b51ce9c834624a7c3ff603c41e85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "check_point_1\n",
      "check_point_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d14551c28049ae82b0e221f4ab00f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=79.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.24023\n"
     ]
    }
   ],
   "source": [
    "dictionary2 = net(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6717"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clon', '복제'),\n",
       " ('spanish', '스페인어'),\n",
       " ('fate', '운명'),\n",
       " ('jong-min', '김종민'),\n",
       " ('dynamic', '역동적'),\n",
       " ('bright', '밝'),\n",
       " ('lazarus', '나사'),\n",
       " ('auction', '옥션'),\n",
       " ('logo', '로고'),\n",
       " ('founder', '설립자')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-992763137a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m54621\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "[(net.vocs.id2word[w[0].item()],net.vocs.id2word[w[1].item()+54621]) for w in dictionary[3000:3100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.vocs.id2word[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--seed SEED] [--verbose VERBOSE]\n",
      "                             [--exp_path EXP_PATH] [--exp_name EXP_NAME]\n",
      "                             [--exp_id EXP_ID] [--export EXPORT]\n",
      "                             [--src_lang SRC_LANG] [--tgt_lang TGT_LANG]\n",
      "                             [--emb_dim EMB_DIM] [--max_vocab MAX_VOCAB]\n",
      "                             [--n_refinement N_REFINEMENT]\n",
      "                             [--dico_train DICO_TRAIN] [--dico_eval DICO_EVAL]\n",
      "                             [--dico_method DICO_METHOD]\n",
      "                             [--dico_build DICO_BUILD]\n",
      "                             [--dico_threshold DICO_THRESHOLD]\n",
      "                             [--dico_max_rank DICO_MAX_RANK]\n",
      "                             [--dico_min_size DICO_MIN_SIZE]\n",
      "                             [--dico_max_size DICO_MAX_SIZE]\n",
      "                             [--src_emb SRC_EMB] [--tgt_emb TGT_EMB]\n",
      "                             [--normalize_embeddings NORMALIZE_EMBEDDINGS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/john/.local/share/jupyter/runtime/kernel-de2c415e-e0be-43cb-8d71-cfce216df044.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Supervised training')\n",
    "parser.add_argument(\"--seed\", type=int, default=-1, help=\"Initialization seed\")\n",
    "parser.add_argument(\"--verbose\", type=int, default=2, help=\"Verbose level (2:debug, 1:info, 0:warning)\")\n",
    "parser.add_argument(\"--exp_path\", type=str, default=\"\", help=\"Where to store experiment logs and models\")\n",
    "parser.add_argument(\"--exp_name\", type=str, default=\"debug\", help=\"Experiment name\")\n",
    "parser.add_argument(\"--exp_id\", type=str, default=\"\", help=\"Experiment ID\")\n",
    "#parser.add_argument(\"--cuda\", type=bool_flag, default=True, help=\"Run on GPU\")\n",
    "parser.add_argument(\"--export\", type=str, default=\"txt\", help=\"Export embeddings after training (txt / pth)\")\n",
    "\n",
    "# data\n",
    "parser.add_argument(\"--src_lang\", type=str, default='en', help=\"Source language\")\n",
    "parser.add_argument(\"--tgt_lang\", type=str, default='es', help=\"Target language\")\n",
    "parser.add_argument(\"--emb_dim\", type=int, default=300, help=\"Embedding dimension\")\n",
    "parser.add_argument(\"--max_vocab\", type=int, default=200000, help=\"Maximum vocabulary size (-1 to disable)\")\n",
    "# training refinement\n",
    "parser.add_argument(\"--n_refinement\", type=int, default=5, help=\"Number of refinement iterations (0 to disable the refinement procedure)\")\n",
    "# dictionary creation parameters (for refinement)\n",
    "parser.add_argument(\"--dico_train\", type=str, default=\"default\", help=\"Path to training dictionary (default: use identical character strings)\")\n",
    "parser.add_argument(\"--dico_eval\", type=str, default=\"default\", help=\"Path to evaluation dictionary\")\n",
    "parser.add_argument(\"--dico_method\", type=str, default='csls_knn_10', help=\"Method used for dictionary generation (nn/invsm_beta_30/csls_knn_10)\")\n",
    "parser.add_argument(\"--dico_build\", type=str, default='S2T&T2S', help=\"S2T,T2S,S2T|T2S,S2T&T2S\")\n",
    "parser.add_argument(\"--dico_threshold\", type=float, default=0, help=\"Threshold confidence for dictionary generation\")\n",
    "parser.add_argument(\"--dico_max_rank\", type=int, default=10000, help=\"Maximum dictionary words rank (0 to disable)\")\n",
    "parser.add_argument(\"--dico_min_size\", type=int, default=0, help=\"Minimum generated dictionary size (0 to disable)\")\n",
    "parser.add_argument(\"--dico_max_size\", type=int, default=0, help=\"Maximum generated dictionary size (0 to disable)\")\n",
    "# reload pre-trained embeddings\n",
    "parser.add_argument(\"--src_emb\", type=str, default='', help=\"Reload source embeddings\")\n",
    "parser.add_argument(\"--tgt_emb\", type=str, default='', help=\"Reload target embeddings\")\n",
    "parser.add_argument(\"--normalize_embeddings\", type=str, default=\"\", help=\"Normalize embeddings before training\")\n",
    "\n",
    "\n",
    "# parse parameters\n",
    "#params_added = parser.parse_args()\n",
    "params_added = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['을', '는', '에', '은', '를', 'ㄴ', '가', '한', '고', '있', '로', '으로', '에서', '년', '들', '과', '했', '그', '일', '하는', '해', '것', 'ㅓㅆ', '도', '나', '하였', '월', '한다', '되었', 'ㅓ', 'ㄹ', '게', '등', '된', '들이', '대', '습니다', '사람', '하여', '며', 'ㅂ니다', '에게', '면', '에는', '것이', '지만', '히', '으며', '되', '때', '었다', '말', 'ㅆ', '없', '어', '우리', '다고', '스', '자', '만', '하게', '던', '라고', '많', '그러', '까지', '라', '명', '중', '같', '된다', '주', '제', '다는', '더', '대한', '하기', '되어', '라는']\n"
     ]
    }
   ],
   "source": [
    "print([net.vocs.id2word[i] for i in range(54621,54700)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9603e-02,  1.4521e-01, -1.2104e-01, -1.1321e-01, -1.4293e-01,\n",
       "          2.2628e-02, -1.1703e-01, -1.5021e-02, -1.4876e-01,  7.0379e-02,\n",
       "         -1.7221e-01,  1.3113e-01,  1.3274e-01,  1.0571e-01, -1.6291e-01,\n",
       "          5.0270e-02, -2.5549e-01,  7.2935e-02,  1.6524e-01, -3.7083e-01,\n",
       "          7.4613e-02, -4.9522e-02, -3.1357e-01, -5.8221e-02,  1.7784e-01,\n",
       "          2.1596e-01,  4.0310e-01,  1.7364e-02,  4.9810e-02,  3.0469e-03,\n",
       "          2.2192e-02, -5.3297e-01, -7.7074e-02, -5.9651e-02,  5.1684e-01,\n",
       "          9.2652e-02,  2.2629e-01,  3.4512e-02, -5.8818e-01, -3.2606e-02,\n",
       "         -6.0538e-02,  1.1353e-01, -1.2166e-01, -3.1682e-02,  1.1525e-01,\n",
       "          1.1659e-01,  2.1780e-01, -4.6682e-02, -1.2379e-01, -1.0832e-01,\n",
       "         -5.6783e-03,  4.9250e-02, -1.9953e-01,  1.5720e-01,  2.0204e-01,\n",
       "          1.8510e-01,  3.1060e-01, -1.9300e-01,  3.2811e-01, -1.3091e-01,\n",
       "          1.2648e-01, -3.1665e-01,  9.6038e-02, -1.1138e-01,  2.4818e-02,\n",
       "          2.4642e-02,  2.2964e-01, -3.2697e-02,  1.1481e-01, -1.7719e-02,\n",
       "          3.0752e-02, -1.1099e-02, -7.9432e-02,  1.0103e-01,  1.4876e-01,\n",
       "         -1.9717e-01,  3.5005e-03, -9.7219e-02, -2.2460e-03,  7.6716e-02,\n",
       "          8.6952e-02,  9.6725e-02, -9.0594e-02,  1.2309e-01, -1.4278e-01,\n",
       "         -4.6345e-02,  2.7062e-01,  6.2847e-02,  1.4224e-01, -5.7543e-02,\n",
       "         -3.9731e-03,  4.7715e-02, -1.4865e-01,  8.0369e-03,  1.2432e-01,\n",
       "         -1.1675e-01,  1.8097e-01, -1.5016e-01, -3.5083e-01,  1.2167e-01,\n",
       "          2.3532e-01,  8.4783e-03,  1.1175e-02, -1.6908e-02, -2.1850e-01,\n",
       "          1.3029e-02,  1.5105e-01, -2.0855e-01,  6.7300e-02,  9.2775e-02,\n",
       "          2.2629e-02, -8.8187e-02, -1.6884e-02,  5.2185e-02,  5.6878e-01,\n",
       "          1.3837e-02,  1.5586e-01, -5.2678e-02,  2.3713e-01, -5.2802e-02,\n",
       "         -2.6866e-01, -6.9844e-02,  3.6047e-01,  1.5148e-01,  5.3335e-02,\n",
       "          9.8592e-02,  2.1547e-02,  1.3745e-01, -3.2587e-01, -1.2870e-01,\n",
       "          1.2500e-01,  2.3925e-01, -1.5249e-01,  2.0537e-01, -2.4786e-01,\n",
       "         -9.8257e-02, -5.6806e-02,  4.6687e-02,  1.3498e-01,  1.2430e-01,\n",
       "         -1.7015e-01,  1.2621e-01, -1.1392e-01, -2.6038e-01,  8.1110e-02,\n",
       "          1.2278e-01, -4.7448e-03, -1.3902e-01, -1.1152e-01, -1.1036e-01,\n",
       "          8.9265e-03, -1.3356e-01, -5.9141e-02, -3.3304e-03,  2.7943e-01,\n",
       "          2.3993e-01,  8.7147e-02,  6.7074e-02,  2.3652e-02, -5.2348e-02,\n",
       "         -7.6972e-03,  8.9925e-03, -2.5035e-01,  1.2949e-01,  1.3007e-01,\n",
       "         -4.1840e-01,  2.5694e-01, -6.0724e-02,  1.3846e-01, -7.5697e-02,\n",
       "         -5.5379e-03, -5.7854e-02,  2.4477e-02, -2.6308e-03,  1.0429e-01,\n",
       "         -1.6545e-01, -3.0054e-01,  3.0457e-01, -1.1251e-02, -7.4583e-02,\n",
       "         -1.8642e-01,  4.6209e-02,  1.2808e-01,  3.8791e-01,  2.0029e-02,\n",
       "          1.0588e-01,  9.1225e-02,  2.3408e-02,  1.4655e-01, -1.2041e-01,\n",
       "         -3.4347e-02,  9.6758e-02, -1.3573e-01, -8.0780e-02, -1.0245e-01,\n",
       "          4.0601e-02, -8.5630e-02,  4.3540e-02,  7.8969e-02, -1.5723e-01,\n",
       "          3.0013e-02, -2.3062e-01, -1.8354e-01, -2.2284e-01,  1.5756e-02,\n",
       "         -2.9054e-01,  2.2961e-01,  2.4641e-01, -1.0577e-01, -4.3991e-02,\n",
       "          1.0940e-01, -8.8366e-02, -3.8971e-02,  7.2242e-02,  1.1624e-01,\n",
       "          3.5105e-01,  1.3777e-02, -5.3819e-03, -2.3176e-01, -7.6214e-02,\n",
       "          2.5025e-02, -1.6365e-01,  2.6060e-02,  1.1190e-01, -2.4617e-02,\n",
       "          2.7089e-02, -3.8702e-02, -3.1159e-02,  7.8012e-02, -2.3112e-02,\n",
       "         -2.7791e-02, -3.3151e-02,  2.7376e-01,  2.8169e-01,  3.6475e-01,\n",
       "         -5.6322e-02, -1.5764e-01,  1.5776e-01, -1.8369e-01, -1.6175e-01,\n",
       "         -6.9371e-02, -1.2096e-02, -1.5701e-01,  1.2975e-01,  2.0119e-01,\n",
       "          2.3248e-01,  2.1859e-01,  3.2851e-01,  2.6858e-01, -5.6533e-02,\n",
       "         -1.6337e-01,  4.2802e-02,  1.5367e-04, -5.4418e-02,  8.1252e-02,\n",
       "          3.8152e-02,  5.9816e-02, -1.4505e-01, -9.0132e-02,  1.3585e-01,\n",
       "          2.6437e-01,  1.5538e-01,  4.5693e-01, -1.1919e-02,  1.7618e-02,\n",
       "         -3.7593e-01, -2.2269e-01,  8.9814e-02, -7.5290e-02, -5.8453e-02,\n",
       "          3.0011e-01,  2.0334e-01, -4.8941e-02, -3.5781e-02, -1.0178e-02,\n",
       "          8.4723e-02, -9.4945e-03, -1.5596e-01, -5.2698e-02, -1.6622e-01,\n",
       "         -2.1008e-01, -6.2516e-02,  4.9855e-02, -2.5175e-02, -4.7517e-02,\n",
       "         -4.2202e-01, -4.5596e-01, -3.2921e-02,  9.1421e-02,  1.3773e-01,\n",
       "          1.2888e-01, -1.2053e-01, -3.6805e-02,  4.0435e-02,  3.1753e-01,\n",
       "         -2.2402e-01, -6.5101e-03, -8.3899e-02,  1.4121e-01,  2.1537e-02],\n",
       "        [-6.2548e-02, -1.7008e-01,  2.9577e-02, -7.2410e-02, -1.8618e-01,\n",
       "          8.4058e-02, -2.8895e-01,  5.5587e-02,  4.8013e-02, -1.3800e-01,\n",
       "         -4.2670e-02, -2.2550e-01, -1.1815e-01,  9.3635e-02,  2.8127e-01,\n",
       "         -1.6713e-01, -6.3313e-01, -1.7586e-01,  6.1349e-02, -1.4932e-01,\n",
       "          2.2870e-01,  2.2551e-02, -1.9664e-01, -4.8755e-02,  1.4532e-01,\n",
       "          1.0038e-03, -3.3115e-02,  8.8854e-02, -5.8113e-02,  2.7030e-01,\n",
       "          3.0171e-02, -9.8884e-02, -2.2882e-01, -2.2059e-01,  2.0674e-01,\n",
       "         -3.9651e-02,  2.5228e-01,  6.0279e-02, -1.9081e-01,  5.7457e-02,\n",
       "          1.3157e-01, -1.0071e-01,  1.5169e-02, -8.0468e-03, -2.4106e-01,\n",
       "         -8.4708e-02, -5.6139e-02, -1.5555e-01,  1.0671e-01,  2.4675e-01,\n",
       "         -2.4988e-02, -2.1389e-01,  1.4293e-01, -8.8718e-02, -9.9244e-04,\n",
       "         -1.0443e-01, -1.2083e-01, -6.0822e-02,  4.8844e-02, -5.4592e-02,\n",
       "         -2.3798e-01,  5.2185e-02,  4.5940e-01, -2.3147e-01,  6.7269e-02,\n",
       "         -1.1119e-01, -1.5864e-01, -5.1924e-02, -3.9337e-02,  1.4951e-01,\n",
       "         -1.3522e-01,  1.9238e-01,  2.2224e-01,  8.0871e-02, -1.0716e-01,\n",
       "         -3.2706e-02,  1.6966e-01, -5.4506e-02,  2.4433e-02, -6.1379e-03,\n",
       "          3.2912e-02, -6.0619e-02,  4.0455e-01,  4.5424e-01,  1.2409e-01,\n",
       "         -1.5440e-03, -1.1485e-01, -2.1353e-01, -1.7086e-01,  3.8333e-01,\n",
       "          1.9461e-02, -1.8056e-01,  3.1140e-02, -1.2154e-01,  1.4784e-01,\n",
       "         -1.0414e-01, -6.2871e-02, -1.4134e-01, -8.5210e-02, -3.6484e-02,\n",
       "         -5.9920e-02, -1.7827e-01,  1.5476e-01, -9.9277e-02,  1.8273e-01,\n",
       "          1.9395e-01,  1.8402e-02, -1.3200e-01,  7.8568e-02, -1.7938e-01,\n",
       "          1.7132e-01, -2.3333e-01,  1.8926e-02, -2.6474e-01,  4.0612e-01,\n",
       "         -1.5999e-01,  1.6994e-01,  2.4168e-02, -9.2998e-02,  1.7637e-01,\n",
       "          2.1080e-01,  5.9198e-02,  9.9984e-02,  3.9317e-02,  5.0126e-02,\n",
       "          1.0345e-01, -1.0842e-01,  6.2809e-01, -2.6186e-01,  1.2417e-01,\n",
       "          4.4597e-02, -5.8404e-02, -6.1478e-02,  1.4547e-01,  7.0729e-02,\n",
       "         -8.2764e-02,  1.7158e-01,  4.4001e-02,  2.9111e-01, -1.2345e-02,\n",
       "         -1.4755e-01,  3.4135e-02,  1.4036e-01, -2.2337e-02,  1.2496e-01,\n",
       "         -1.5090e-01,  1.1340e-02, -3.5019e-04, -5.4385e-02,  3.3125e-02,\n",
       "          1.2743e-01,  2.0171e-01, -2.9055e-02,  6.1044e-03,  2.5616e-01,\n",
       "         -8.8295e-02, -3.4658e-01,  1.5470e-01,  1.7040e-01,  8.2646e-02,\n",
       "          3.0293e-01, -1.0965e-01, -1.4818e-01,  1.7844e-01, -1.9940e-01,\n",
       "         -2.3120e-01,  2.2346e-01,  1.0382e-01, -1.4380e-01, -1.3089e-01,\n",
       "          1.8815e-01, -1.6345e-01,  3.6726e-02, -6.7470e-02, -1.9172e-02,\n",
       "          1.1977e-01,  2.9655e-02,  5.5788e-01, -1.6956e-01,  2.7171e-01,\n",
       "         -1.0637e-01, -1.0765e-01, -4.4727e-02, -1.1747e-02, -2.4028e-02,\n",
       "          7.7859e-02, -2.2632e-01, -7.3575e-02,  1.2706e-01, -1.5873e-01,\n",
       "          1.5536e-01,  8.5909e-03,  5.0038e-03,  1.6949e-02,  4.9478e-02,\n",
       "         -4.4123e-02, -1.0248e-01, -1.0146e-01,  5.3468e-02, -1.4594e-01,\n",
       "          4.9356e-02, -5.4204e-02,  5.6336e-02, -1.4314e-02,  1.1617e-01,\n",
       "         -4.7887e-01,  1.7769e-01,  3.3322e-02, -7.7808e-02, -6.7809e-02,\n",
       "         -1.2410e-01, -3.3914e-03, -2.3902e-02,  5.4329e-02, -1.2820e-01,\n",
       "          3.3753e-01, -1.0396e-01, -2.4977e-01, -2.4832e-01,  3.6262e-02,\n",
       "         -9.6051e-03, -7.7759e-02, -5.2245e-02, -9.1309e-02,  2.2950e-01,\n",
       "         -7.4881e-02, -1.0515e-01,  2.6633e-02, -8.0094e-02, -1.5165e-01,\n",
       "          1.3051e-01, -9.9396e-02, -8.4721e-02, -2.2645e-01,  1.7193e-01,\n",
       "         -1.9516e-01, -1.1884e-01,  7.5699e-02,  6.4991e-02, -9.7842e-02,\n",
       "          3.0011e-01,  5.5016e-02, -2.0198e-01,  3.0620e-02, -6.4415e-02,\n",
       "         -1.5578e-01, -1.2379e-01,  6.6821e-02,  1.2199e-01,  5.3998e-02,\n",
       "          9.4678e-02,  2.9596e-02, -7.6383e-02, -2.7960e-02,  2.7068e-01,\n",
       "         -3.9323e-02, -1.2104e-01,  1.7707e-01,  2.4285e-02, -1.2647e-01,\n",
       "          5.1423e-02, -1.0662e-01,  4.4094e-01,  1.0135e-01, -4.6483e-02,\n",
       "         -1.2744e-01, -1.6378e-01, -2.6090e-01,  4.3559e-01, -8.6304e-02,\n",
       "         -8.2854e-02,  1.3057e-01, -1.6478e-02,  2.0282e-01,  1.1843e-01,\n",
       "          1.2231e-01, -7.8697e-02, -1.4712e-01,  2.2933e-01, -8.4089e-02,\n",
       "         -8.4677e-02, -1.0782e-01, -1.3598e-01, -1.1115e-01,  7.0768e-02,\n",
       "          3.0938e-02, -8.8592e-02, -3.7130e-02,  5.0743e-02, -3.2735e-02,\n",
       "          1.7431e-01, -2.6365e-02,  2.6831e-01, -2.3929e-02,  1.4312e-01,\n",
       "         -3.0408e-01, -3.9755e-01, -7.3040e-02,  2.3152e-01, -1.7304e-02]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.emb[54621:54623]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return sum(a*b)/((sum(a*a)**.5)*(sum(b*b)**.5))\n",
    "\n",
    "wD = params['vocab'].vocs\n",
    "emb = params['state_dict']['model_embeddings.vocabs.weight']\n",
    "w = \"man men woman women\".split(' ')\n",
    "mapped = emb[wD[w[0]]] - emb[wD[w[2]]] + emb[wD[w[3]]]\n",
    "aa = torch.tensor([cos_sim(emb[i],mapped) for i in tqdm(range(3,10000))])  \n",
    "\n",
    "aa.size(), cos_sim(emb[wD['queen']],emb[wD['queen']])\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute 'spatial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-1468ebc86b11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy' has no attribute 'spatial'"
     ]
    }
   ],
   "source": [
    "scipy.spatial.distance.cosine(src[0],tgt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = '../../../preProject/NMT/preproc_Module/inputs/'\n",
    "\n",
    "with open(path+'dict_enT.json','r') as f:\n",
    "    dictE = json.load(f)\n",
    "with open(path+'dict_koT.json','r') as f:\n",
    "    dictK = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0367,  0.0331, -0.0166,  ..., -0.0038,  0.0020,  0.0333],\n",
       "         [ 0.0051, -0.0550,  0.0519,  ...,  0.0196, -0.0466,  0.0145],\n",
       "         [-0.0470,  0.0396,  0.0391,  ...,  0.0435, -0.0558, -0.0068],\n",
       "         ...,\n",
       "         [-0.0227, -0.0017,  0.0534,  ...,  0.0348, -0.0530, -0.0424],\n",
       "         [ 0.0522, -0.0373, -0.0069,  ..., -0.0184,  0.0200, -0.0478],\n",
       "         [-0.0366, -0.0521, -0.0287,  ..., -0.0114,  0.0231, -0.0196]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(params, 300)\n",
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0448, -0.0223,  0.0343,  ..., -0.0447, -0.0565, -0.0145],\n",
       "         [ 0.0219, -0.0308, -0.0284,  ...,  0.0432,  0.0363,  0.0151],\n",
       "         [ 0.0056,  0.0464, -0.0525,  ..., -0.0214, -0.0279,  0.0573],\n",
       "         ...,\n",
       "         [-0.0279,  0.0163,  0.0007,  ..., -0.0255, -0.0531, -0.0169],\n",
       "         [ 0.0162,  0.0311, -0.0370,  ...,  0.0029,  0.0508, -0.0155],\n",
       "         [-0.0564, -0.0338, -0.0108,  ..., -0.0028, -0.0249, -0.0358]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3189, grad_fn=<L1LossBackward>),\n",
       " tensor(0.3399, grad_fn=<L1LossBackward>),\n",
       " tensor(0.3080, grad_fn=<L1LossBackward>),\n",
       " tensor(0.3041, grad_fn=<L1LossBackward>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_words = [(w,sorted(v, key=lambda x:x[1][0], reverse = True)[0][0]) for w,v in dictE.items()]\n",
    "bi_words += [(sorted(v, key=lambda x:x[1][0], reverse = True)[0][0],w) for w,v in dictE.items()]\n",
    "net = Net(params, bi_words, 300)\n",
    "net(['president'],['대통령']), net(['personal'],['개인']), net(['china'],['중국']), net(['korea'],['한국'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3520, grad_fn=<L1LossBackward>),\n",
       " tensor(0.3822, grad_fn=<L1LossBackward>),\n",
       " tensor(0.3953, grad_fn=<L1LossBackward>),\n",
       " tensor(0.2889, grad_fn=<L1LossBackward>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net = Net(params, 300)\n",
    "net(['president'],['대통령']), net(['personal'],['개인']), net(['information'],['중국']), net(['china'],['중국'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3520, grad_fn=<L1LossBackward>),\n",
       " tensor(0.3355, grad_fn=<L1LossBackward>),\n",
       " tensor(0.3939, grad_fn=<L1LossBackward>),\n",
       " tensor(0.2889, grad_fn=<L1LossBackward>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(['president'],['대통령']), net(['korea'],['한국']), net(['information'],['미국']), net(['china'],['중국'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_words = [(w,sorted(v, key=lambda x:x[1][0], reverse = True)[0][0]) for w,v in dictE.items()]\n",
    "bi_words += [(sorted(v, key=lambda x:x[1][0], reverse = True)[0][0],w) for w,v in dictE.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('personal', '개인적'),\n",
       " ('computing', '컴퓨팅'),\n",
       " ('can', '수'),\n",
       " ('you', '당신'),\n",
       " ('mention', '언급'),\n",
       " ('few', '몇'),\n",
       " ('wireless', '무선'),\n",
       " ('optical', '광학'),\n",
       " ('mouse', '마우스'),\n",
       " ('another', '또')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " bi_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.5046, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.2867, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.5290, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.4477, grad_fn=<DivBackward0>)])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(params, 300)\n",
    "beta = 0.1\n",
    "src = [e[0] for e in bi_words]\n",
    "tgt = [e[1] for e in bi_words]\n",
    "M = net.procrustes(src,tgt)\n",
    "U, S, V_t = scipy.linalg.svd(M, full_matrices=True)\n",
    "W = U.dot(V_t)\n",
    "#W = (1+beta)*W - beta*((W@W.transpose(0,1))@W)\n",
    "#W = W@W.transpose(0,1)\n",
    "net.w_map.weight.data = torch.tensor(W)\n",
    "net(['president'],['대통령']), net(['personal'],['개인']), net(['china'],['중국']), net(['korea'],['한국'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000000e+00,  2.3469329e-07,  8.8172965e-07, ...,\n",
       "         6.2212348e-07,  5.5879354e-08, -5.8300793e-07],\n",
       "       [-2.2538006e-07,  1.0000001e+00,  4.5681372e-07, ...,\n",
       "        -2.2351742e-07, -1.6950071e-07,  3.1292439e-07],\n",
       "       [-7.2410330e-07, -5.4575503e-07,  1.0000002e+00, ...,\n",
       "        -7.9534948e-07,  1.0337681e-06,  9.0617687e-07],\n",
       "       ...,\n",
       "       [-4.9173832e-07,  2.9988587e-07,  6.2771142e-07, ...,\n",
       "         1.0000002e+00,  2.9336661e-08,  2.9616058e-07],\n",
       "       [-2.1792948e-07,  3.1478703e-07, -6.2957406e-07, ...,\n",
       "        -5.2619725e-08,  1.0000001e+00, -1.8323772e-07],\n",
       "       [ 4.4889748e-07, -2.3655593e-07, -6.6962093e-07, ...,\n",
       "        -6.8917871e-08, -8.1490725e-08,  1.0000002e+00]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W@W.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "([tensor(0.5046, grad_fn=<DivBackward0>)],\n",
    " [tensor(0.2867, grad_fn=<DivBackward0>)],\n",
    " [tensor(0.5290, grad_fn=<DivBackward0>)],\n",
    " [tensor(0.4477, grad_fn=<DivBackward0>)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.00001984834671+0j)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = scipy.linalg.eigvals(W)\n",
    "sum(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-12.239889, 157415.62, -18.867588, 29.56411, 300.00006)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.sum(), S.sum(), V_t.sum(), W.sum(), W.dot(W.transpose(1,0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "complex"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(10.00001984834671+0j) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.0737, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.1303, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.4727, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.4477, grad_fn=<DivBackward0>)])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(['china'],['대통령']), net(['personal'],['중국']), net(['information'],['정보']), net(['korea'],['한국'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.3361, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.2289, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.3762, grad_fn=<DivBackward0>)],\n",
       " [tensor(0.4073, grad_fn=<DivBackward0>)])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(['optical'],['광학'] ), net(['trump'],['오바마'] ), net(['obama'],['오바마'] ), net(['wireless'],['무선'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a833aa167e4499b32cd2caa8c6c929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "b_size = 32\n",
    "beta = 0.01\n",
    "\n",
    "net = Net(params, 300)\n",
    "net.train()\n",
    "\n",
    "uniform_init = 0.1\n",
    "\n",
    "for p in net.parameters():\n",
    "    p.data.uniform_(-uniform_init, uniform_init)\n",
    "   \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "batch_num = len(bi_words) // b_size +1\n",
    "for ik in tqdm(range(100)):\n",
    "            \n",
    "    for i in range(batch_num):\n",
    "        src,tgt = next(batch_iter(bi_words,b_size))\n",
    "        optimizer.zero_grad()\n",
    "        loss = net(src,tgt)       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print(list(net.parameters())[0])\n",
    "    #print('complete on epoch')  \n",
    "    with torch.no_grad():\n",
    "        W = net.w_map.weight.data\n",
    "        U, S, V_t = scipy.linalg.svd(W, full_matrices=True)\n",
    "        W = U@V_t\n",
    "        #list(net.parameters())[0]\n",
    "        W = (1+beta)*W - beta*((W@W.transpose(0,1))@W)\n",
    "        net.w_map.weight.data = torch.tensor(W)\n",
    "        #for p in net.parameters():\n",
    "        #    p.data = W\n",
    "            #p.data.requires_grad_()\n",
    "\n",
    "            #p.data.requires_grad_()\n",
    "\n",
    "    #print(list(net.parameters())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for p in net.parameters():\n",
    "        p.data = W\n",
    "        p.data.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n"
     ]
    }
   ],
   "source": [
    "list(net.parameters())[0], \n",
    "for p in net.parameters():\n",
    "    print(len(p.data), len(p.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0310, -0.0017,  0.0171,  ..., -0.0174, -0.0127, -0.0078],\n",
       "        [ 0.0254, -0.0097, -0.0098,  ..., -0.0167,  0.0122, -0.0286],\n",
       "        [ 0.0228, -0.0086,  0.0436,  ...,  0.0654, -0.0272,  0.0072],\n",
       "        ...,\n",
       "        [ 0.0241,  0.0039,  0.0124,  ...,  0.0379,  0.0423, -0.0138],\n",
       "        [ 0.0009, -0.0169, -0.0552,  ...,  0.0178,  0.0275, -0.0162],\n",
       "        [ 0.0173, -0.0218,  0.0376,  ..., -0.0193, -0.0095, -0.0664]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = list(net.parameters())[0].requires_grad_(False)\n",
    "W = (1+0.01)*W -0.01*(W@W.transpose(0,1)@W)\n",
    "for p in net.parameters():\n",
    "    p.data = W\n",
    "list(net.parameters())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(net.parameters())) #[0].requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0310, -0.0017,  0.0171,  ..., -0.0174, -0.0127, -0.0078],\n",
       "        [ 0.0254, -0.0097, -0.0098,  ..., -0.0167,  0.0122, -0.0286],\n",
       "        [ 0.0228, -0.0086,  0.0436,  ...,  0.0654, -0.0272,  0.0072],\n",
       "        ...,\n",
       "        [ 0.0241,  0.0039,  0.0124,  ...,  0.0379,  0.0423, -0.0138],\n",
       "        [ 0.0009, -0.0169, -0.0552,  ...,  0.0178,  0.0275, -0.0162],\n",
       "        [ 0.0173, -0.0218,  0.0376,  ..., -0.0193, -0.0095, -0.0664]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    p.data = W\n",
    "list(net.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2289, grad_fn=<L1LossBackward>),\n",
       " tensor(0.2958, grad_fn=<L1LossBackward>),\n",
       " tensor(0.3492, grad_fn=<L1LossBackward>),\n",
       " tensor(0.3658, grad_fn=<L1LossBackward>),\n",
       " tensor(0.2329, grad_fn=<L1LossBackward>))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(['president'],['대통령']), net(['information'],['정보']), net(['personal'],['다중']), net(['china'],['교환']),net(['korea'],['한국'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['한국', [0.50732081031218, 199807, 1.7897433749406606]],\n",
       " ['북한', [0.36813596455527875, 208786, 1.5903648838103746]],\n",
       " ['국내', [0.24133897939656412, 163304, 3.6500185084996724]],\n",
       " ['일', [0.2236404031691881, 522615, 0.3249879573054788]],\n",
       " ['자유한국당', [0.21126844827059588, 155178, 4.7488237691253286]]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictE['korea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-dd0cd9a93620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(bi_words,b_size):\n",
    "    batch_size = b_size\n",
    "    batch_num = math.ceil(len(bi_words) / batch_size)\n",
    "    index_array = list(range(len(bi_words)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
    "        examples = [bi_words[idx] for idx in indices]\n",
    "        src = [e[0] for e in examples]\n",
    "        tgt = [e[1] for e in examples]\n",
    "\n",
    "        yield src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(bi_words,key=lambda x:x[0],reverse=True)[1200:1250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'참': 0.7092478180651339, '사실': 0.1424496491409793, '숭배': 0.14830253279388678}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dictX ={}\n",
    "for w,v in dictE.items():\n",
    "    dX = {}\n",
    "    for wt in v:\n",
    "        dX[wt[0]] = wt[1][0]/(1+np.abs(np.log(wt[1][2])))\n",
    "    denome = sum(dX.values())\n",
    "    dictX[w] = {k:v/denome for k,v in dX.items()}\n",
    "    \n",
    "dictX['true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'중국': 0.8330298871257, '미국': 0.10416938597051914, '무역전쟁': 0.06280072690378084}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictX['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15082288973458366, 0.28517894223366247)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.abs(np.log(0.86)), np.abs(np.log(1.33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['전', [0.6036746244441473, 190080, 0.4297426794136009]],\n",
       " ['검찰', [0.13637146344892434, 90863, 1.6938333827453305]],\n",
       " ['혐의', [0.13075522730706107, 83887, 2.1354937579427373]],\n",
       " ['이전', [0.12848188311517145, 73686, 3.4515193620491753]],\n",
       " ['대통령', [0.10423307560107185, 160728, 0.5515034509387519]]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictE['former']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('많', 'many'), ('개인적', 'personal'), ('컴퓨팅', 'computing'), ('수', 'can'), ('우리', 'our'), ('어떻게', 'how'), ('가능', 'possible'), ('당신', 'you'), ('나', 'i'), ('그', 'he'), ('일', 'th'), ('올해', 'year'), ('때문', 'because'), ('언급', 'mention'), ('몇', 'few'), ('소수', 'minority'), ('전', 'former'), ('무선', 'wireless'), ('광학', 'optical'), ('쥐', 'rats'), ('마우스', 'mouse'), ('다르', 'other'), ('또', 'another'), ('모든', 'all'), ('모두', 'all'), ('지만', 'but'), ('않', 'not'), ('아니', 'not'), ('또한', 'also'), ('필요', 'need'), ('책상', 'desk'), ('데스크', 'desk'), ('센서', 'sensor'), ('감지', 'detect'), ('통제', 'control'), ('제어', 'control'), ('운동', 'movement'), ('독립운동', 'independence'), ('손목', 'wrist'), ('팔', 'arm'), ('통', 'through'), ('공기', 'air'), ('항공', 'aviation'), ('공기청정기', 'purifier'), ('오염', 'pollution'), ('인공지능', 'artificial'), ('정보', 'information'), ('지성', 'intelligent'), ('국정원', 'nis'), ('관계자', 'official'), ('공식', 'official'), ('관리', 'management'), ('공개', 'unveil'), ('음모', 'conspiracy'), ('선박', 'ship'), ('동남아', 'southeast'), ('동남아시아', 'southeast'), ('남동부', 'southeastern'), ('아시아', 'asia'), ('동북아', 'northeast'), ('경고', 'warn'), ('좁', 'narrow'), ('좁히', 'narrow'), ('물', 'water'), ('거', 'almost'), ('사람', 'people'), ('하나', 'one'), ('세계', 'world'), ('세상', 'world'), ('월드컵', 'cup'), ('해양', 'marine'), ('무역', 'trade'), ('중국', 'china'), ('거래', 'transaction'), ('공정위', 'ftc'), ('민주노총', 'confederat'), ('취약', 'vulnerable'), ('취약계층', 'vulnerable'), ('테러', 'terrorism'), ('공격', 'attack'), ('후', 'after'), ('년', 'year'), ('배우', 'actor'), ('학습', 'learn'), ('알카에다', 'qaeda'), ('시도', 'attempt'), ('북한', 'north'), ('트럼프', 'trump'), ('상업', 'commercial'), ('상업적', 'commercial'), ('지역', 'region'), ('전문가', 'expert'), ('전문', 'specializ'), ('네트워크', 'network'), ('망', 'network'), ('여전', 'still'), ('글로벌', 'global'), ('경제', 'economy'), ('경기', 'game'), ('활성화', 'activat')]\n"
     ]
    }
   ],
   "source": [
    "print(bi_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['personal',\n",
       "  'computing',\n",
       "  'can',\n",
       "  'you',\n",
       "  'mention',\n",
       "  'few',\n",
       "  'wireless',\n",
       "  'optical',\n",
       "  'mouse',\n",
       "  'another'],\n",
       " [['개인적', [0.5028736421047497, 36103, 2.2180229967020235]],\n",
       "  ['개인정보', [0.33360332737029397, 28843, 6.2854256125284165]],\n",
       "  ['개인', [0.32997207139147905, 42227, 1.434815199215822]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dictE)[:10],dictE['personal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (w_map): Linear(in_features=300, out_features=300, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net =Net(params, 300)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6918), tensor(335))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'men'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wD.id2word[aa.max(-1)[1].item()+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2850"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wD['queen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4075)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(emb[wD['queen']],mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'king'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wD.id2word[553]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wD.id2word[88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(emb[10],emb[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10.1422), tensor(39.2305), tensor(35.5234))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = params['state_dict']['model_embeddings.vocabs.weight'][5537]\n",
    "bb = params['state_dict']['model_embeddings.vocabs.weight'][129]\n",
    "sum(aa*bb), sum(aa*aa), sum(bb*bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = params['state_dict']['en_gate.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "builtin_function_or_method"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cc.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 300])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = torch.matmul(cc,torch.transpose(cc,0,1))\n",
    "dd.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model_embeddings.vocabs.weight', 'ek_encoder.weight_ih_l0', 'ek_encoder.weight_hh_l0', 'ek_encoder.bias_ih_l0', 'ek_encoder.bias_hh_l0', 'ek_encoder.weight_ih_l0_reverse', 'ek_encoder.weight_hh_l0_reverse', 'ek_encoder.bias_ih_l0_reverse', 'ek_encoder.bias_hh_l0_reverse', 'ek_encoder.weight_ih_l1', 'ek_encoder.weight_hh_l1', 'ek_encoder.bias_ih_l1', 'ek_encoder.bias_hh_l1', 'ek_encoder.weight_ih_l1_reverse', 'ek_encoder.weight_hh_l1_reverse', 'ek_encoder.bias_ih_l1_reverse', 'ek_encoder.bias_hh_l1_reverse', 'ek_decoder.weight_ih', 'ek_decoder.weight_hh', 'ek_decoder.bias_ih', 'ek_decoder.bias_hh', 'ek_h_projection.weight', 'ek_c_projection.weight', 'ek_att_projection.weight', 'ek_combined_output_projection.weight', 'ek_target_vocab_projection.weight', 'ke_encoder.weight_ih_l0', 'ke_encoder.weight_hh_l0', 'ke_encoder.bias_ih_l0', 'ke_encoder.bias_hh_l0', 'ke_encoder.weight_ih_l0_reverse', 'ke_encoder.weight_hh_l0_reverse', 'ke_encoder.bias_ih_l0_reverse', 'ke_encoder.bias_hh_l0_reverse', 'ke_encoder.weight_ih_l1', 'ke_encoder.weight_hh_l1', 'ke_encoder.bias_ih_l1', 'ke_encoder.bias_hh_l1', 'ke_encoder.weight_ih_l1_reverse', 'ke_encoder.weight_hh_l1_reverse', 'ke_encoder.bias_ih_l1_reverse', 'ke_encoder.bias_hh_l1_reverse', 'ke_decoder.weight_ih', 'ke_decoder.weight_hh', 'ke_decoder.bias_ih', 'ke_decoder.bias_hh', 'ke_h_projection.weight', 'ke_c_projection.weight', 'ke_att_projection.weight', 'ke_combined_output_projection.weight', 'ke_target_vocab_projection.weight', 'sub_en_coder.weight_ih_l0', 'sub_en_coder.weight_hh_l0', 'sub_en_coder.bias_ih_l0', 'sub_en_coder.bias_hh_l0', 'en_gate.weight', 'sub_en_projection.weight', 'target_ox_projection.weight'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/john/Notebook/Project2/Translation/nmt_bi_combined_0702\n"
     ]
    }
   ],
   "source": [
    "cd nmt_bi_combined_0702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf nmt_bi_combined_0702_lr0002.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
